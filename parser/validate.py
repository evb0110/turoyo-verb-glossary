#!/usr/bin/env python3
"""
Validation and Verification Tools for Turoyo Data
Generates reports for manual verification
"""

import json
from pathlib import Path
from collections import defaultdict, Counter
import random

class TuroyoValidator:
    def __init__(self, json_path):
        with open(json_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)

        self.verbs = self.data['verbs']
        self.metadata = self.data['metadata']
        self.issues = defaultdict(list)
        self.stats = defaultdict(int)

    def validate_all(self):
        """Run all validation checks"""
        print("üîç Validating extracted data...")

        self.check_completeness()
        self.check_data_quality()
        self.check_etymology()
        self.check_examples()
        self.check_references()
        self.detect_anomalies()

        self.print_report()
        self.generate_verification_samples()

    def check_completeness(self):
        """Check for missing or empty fields"""
        print("  Checking completeness...")

        for verb in self.verbs:
            # Check root
            if not verb.get('root'):
                self.issues['missing_root'].append(verb)

            # Check if cross-reference
            if verb.get('cross_reference'):
                self.stats['cross_references'] += 1
                continue

            # Check etymology
            if not verb.get('etymology'):
                self.issues['missing_etymology'].append(verb['root'])
                self.stats['missing_etymology'] += 1

            # Check stems
            if not verb.get('stems'):
                self.issues['no_stems'].append(verb['root'])
                self.stats['no_stems'] += 1
            else:
                for stem in verb['stems']:
                    # Check forms
                    if not stem.get('forms'):
                        self.issues['no_forms'].append(f"{verb['root']} - {stem['binyan']}")

                    # Check conjugations
                    if not stem.get('conjugations'):
                        self.issues['no_conjugations'].append(f"{verb['root']} - {stem['binyan']}")

    def check_data_quality(self):
        """Check quality of extracted data"""
        print("  Checking data quality...")

        for verb in self.verbs:
            for stem in verb.get('stems', []):
                for conj_type, examples in stem.get('conjugations', {}).items():
                    for example in examples:
                        # Check if Turoyo text exists
                        if not example.get('turoyo'):
                            self.issues['empty_turoyo'].append(
                                f"{verb['root']} - {stem['binyan']} - {conj_type}"
                            )

                        # Check for very short examples (likely parsing errors)
                        turoyo = example.get('turoyo', '')
                        if turoyo and len(turoyo.strip()) < 3:
                            self.issues['short_turoyo'].append(
                                f"{verb['root']}: '{turoyo}'"
                            )

                        # Check translations
                        if not example.get('translations'):
                            self.stats['no_translation'] += 1

                        # Check for suspiciously long text (possible HTML remnants)
                        if len(turoyo) > 1000:
                            self.issues['very_long_example'].append(
                                f"{verb['root']} - {len(turoyo)} chars"
                            )

    def check_etymology(self):
        """Analyze etymology data"""
        print("  Checking etymology...")

        sources = Counter()

        for verb in self.verbs:
            etym = verb.get('etymology')
            if etym:
                source = etym.get('source', 'unknown')
                sources[source] += 1

        self.stats['etymology_sources'] = dict(sources)

    def check_examples(self):
        """Analyze examples"""
        print("  Checking examples...")

        total_examples = 0
        examples_per_verb = []

        for verb in self.verbs:
            verb_examples = 0
            for stem in verb.get('stems', []):
                for conj_type, examples in stem.get('conjugations', {}).items():
                    verb_examples += len(examples)
                    total_examples += len(examples)

            examples_per_verb.append(verb_examples)

        self.stats['total_examples'] = total_examples
        self.stats['avg_examples_per_verb'] = total_examples / len(self.verbs) if self.verbs else 0
        self.stats['max_examples'] = max(examples_per_verb) if examples_per_verb else 0
        self.stats['min_examples'] = min(examples_per_verb) if examples_per_verb else 0

    def check_references(self):
        """Check reference patterns"""
        print("  Checking references...")

        ref_patterns = Counter()

        for verb in self.verbs:
            for stem in verb.get('stems', []):
                for conj_type, examples in stem.get('conjugations', {}).items():
                    for example in examples:
                        for ref in example.get('references', []):
                            # Classify reference type
                            if '/' in ref:
                                ref_patterns['page_reference'] += 1
                            elif ref.isupper():
                                ref_patterns['abbreviation'] += 1
                            elif ref.isdigit():
                                ref_patterns['number_only'] += 1
                            else:
                                ref_patterns['mixed'] += 1

        self.stats['reference_patterns'] = dict(ref_patterns)

    def detect_anomalies(self):
        """Detect potential parsing errors"""
        print("  Detecting anomalies...")

        for verb in self.verbs:
            # Check for verbs with many stems (might be parsing errors)
            if len(verb.get('stems', [])) > 8:
                self.issues['too_many_stems'].append(
                    f"{verb['root']}: {len(verb['stems'])} stems"
                )

            # Check for duplicate binyanim
            binyanim = [s['binyan'] for s in verb.get('stems', [])]
            if len(binyanim) != len(set(binyanim)):
                self.issues['duplicate_binyanim'].append(verb['root'])

    def print_report(self):
        """Print validation report"""
        print("\n" + "="*70)
        print("VALIDATION REPORT")
        print("="*70)

        print("\nüìä STATISTICS:")
        print(f"  Total verbs: {len(self.verbs)}")
        print(f"  Total stems: {sum(len(v.get('stems', [])) for v in self.verbs)}")
        print(f"  Total examples: {self.stats['total_examples']}")
        print(f"  Avg examples/verb: {self.stats['avg_examples_per_verb']:.1f}")
        print(f"  Cross-references: {self.stats['cross_references']}")
        print(f"  Uncertain entries (???): {sum(1 for v in self.verbs if v.get('uncertain'))}")

        print("\nüìö ETYMOLOGY SOURCES:")
        for source, count in sorted(self.stats['etymology_sources'].items(), key=lambda x: -x[1])[:10]:
            print(f"  {source}: {count}")

        print("\n‚ö†Ô∏è  POTENTIAL ISSUES:")
        issue_types = [
            ('missing_etymology', 'Missing etymology'),
            ('no_stems', 'No stems'),
            ('no_forms', 'No forms'),
            ('no_conjugations', 'No conjugations'),
            ('empty_turoyo', 'Empty Turoyo text'),
            ('short_turoyo', 'Suspiciously short Turoyo'),
            ('very_long_example', 'Very long examples'),
            ('too_many_stems', 'Too many stems'),
            ('duplicate_binyanim', 'Duplicate binyanim'),
        ]

        for issue_key, issue_name in issue_types:
            count = len(self.issues.get(issue_key, []))
            if count > 0:
                print(f"  {issue_name}: {count}")

        print("\n" + "="*70)

    def generate_verification_samples(self):
        """Generate random samples for manual verification"""
        output_dir = Path('data/verification')
        output_dir.mkdir(exist_ok=True)

        # Sample 1: Random verbs
        random_sample = random.sample(self.verbs, min(20, len(self.verbs)))
        with open(output_dir / 'random_sample.json', 'w', encoding='utf-8') as f:
            json.dump(random_sample, f, ensure_ascii=False, indent=2)

        # Sample 2: Verbs with most examples
        verbs_with_counts = [
            (v, sum(len(examples) for stem in v.get('stems', [])
                    for examples in stem.get('conjugations', {}).values()))
            for v in self.verbs
        ]
        top_examples = sorted(verbs_with_counts, key=lambda x: -x[1])[:10]
        with open(output_dir / 'top_examples.json', 'w', encoding='utf-8') as f:
            json.dump([v[0] for v in top_examples], f, ensure_ascii=False, indent=2)

        # Sample 3: Verbs with most stems
        verbs_by_stems = sorted(self.verbs, key=lambda v: -len(v.get('stems', [])))[:10]
        with open(output_dir / 'most_stems.json', 'w', encoding='utf-8') as f:
            json.dump(verbs_by_stems, f, ensure_ascii=False, indent=2)

        # Sample 4: Issues for review
        issues_sample = {
            'missing_etymology': self.issues.get('missing_etymology', [])[:10],
            'no_stems': self.issues.get('no_stems', [])[:10],
            'empty_turoyo': self.issues.get('empty_turoyo', [])[:10],
            'short_turoyo': self.issues.get('short_turoyo', [])[:10],
            'too_many_stems': self.issues.get('too_many_stems', [])[:10],
        }

        with open(output_dir / 'issues_sample.json', 'w', encoding='utf-8') as f:
            json.dump(issues_sample, f, ensure_ascii=False, indent=2)

        # Sample 5: Uncertain entries
        uncertain = [v for v in self.verbs if v.get('uncertain')]
        with open(output_dir / 'uncertain_entries.json', 'w', encoding='utf-8') as f:
            json.dump(uncertain, f, ensure_ascii=False, indent=2)

        print(f"\nüìÅ Verification samples saved to: {output_dir}/")
        print("  - random_sample.json (20 random verbs)")
        print("  - top_examples.json (10 verbs with most examples)")
        print("  - most_stems.json (10 verbs with most stems)")
        print("  - issues_sample.json (sample of potential issues)")
        print("  - uncertain_entries.json (all ??? marked entries)")

    def generate_html_report(self):
        """Generate an HTML report for easier manual review"""
        output_file = Path('data/verification/report.html')

        html = f"""
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Turoyo Verb Glossary - Validation Report</title>
    <style>
        body {{ font-family: sans-serif; max-width: 1200px; margin: 20px auto; padding: 0 20px; }}
        h1 {{ color: #333; }}
        .stats {{ background: #f5f5f5; padding: 20px; border-radius: 8px; margin: 20px 0; }}
        .issue {{ background: #fff3cd; padding: 10px; margin: 10px 0; border-left: 4px solid #ffc107; }}
        .success {{ background: #d4edda; padding: 10px; margin: 10px 0; border-left: 4px solid #28a745; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 10px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background: #333; color: white; }}
        .turoyo {{ font-size: 1.1em; color: #0056b3; }}
    </style>
</head>
<body>
    <h1>Turoyo Verb Glossary - Validation Report</h1>

    <div class="stats">
        <h2>Statistics</h2>
        <ul>
            <li>Total verbs: {len(self.verbs)}</li>
            <li>Total stems: {sum(len(v.get('stems', [])) for v in self.verbs)}</li>
            <li>Total examples: {self.stats['total_examples']}</li>
            <li>Average examples per verb: {self.stats['avg_examples_per_verb']:.1f}</li>
        </ul>
    </div>

    <h2>Sample Verbs</h2>
    <table>
        <tr>
            <th>Root</th>
            <th>Etymology</th>
            <th>Stems</th>
            <th>Examples</th>
        </tr>
"""

        # Sample first 20 verbs
        for verb in self.verbs[:20]:
            etym = verb.get('etymology') or {}
            etym_str = f"{etym.get('source', 'N/A')}" if isinstance(etym, dict) else 'N/A'

            num_stems = len(verb.get('stems', []))
            num_examples = sum(len(examples) for stem in verb.get('stems', [])
                               for examples in stem.get('conjugations', {}).values())

            html += f"""
        <tr>
            <td class="turoyo">{verb['root']}</td>
            <td>{etym_str}</td>
            <td>{num_stems}</td>
            <td>{num_examples}</td>
        </tr>
"""

        html += """
    </table>
</body>
</html>
"""

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)

        print(f"  - report.html (HTML overview)")


def main():
    json_file = Path('data/verbs_final.json')

    if not json_file.exists():
        print(f"‚ùå {json_file} not found. Run extract_final.py first.")
        return

    validator = TuroyoValidator(json_file)
    validator.validate_all()
    validator.generate_html_report()

if __name__ == '__main__':
    main()
